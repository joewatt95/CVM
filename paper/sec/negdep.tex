\section{An Unbiased CVM Variant and Negative Dependence\label{sec:negdep}}

An interesting consequence of our invariant-based approach is that it allowed us to devise and verify a refined version of the CVM algorithm that is both total and unbiased.

\subsection{Unbiased CVM Variant}

\begin{algorithm}[t!]
	\caption{New total and unbiased CVM algorithm variant.}\label{alg:cvm_new}
	\begin{algorithmic}[1]
  \Require Stream elements $a_1,\dots,a_l$, $0 < \varepsilon$, $0 < \delta < 1$.
  \Ensure A cardinality estimate $R$ for set $A = \{ a_1,\dots,a_l \}$ such that $\prob \left( |R - |A| | > \varepsilon |A| \right) \leq \delta$
  \State $\chi \gets \{\}, p \gets 1, n = \ceil*{\frac{12}{\varepsilon^2} \ln{(\frac{3l}{\delta})} }$, $\frac{1}{2} \leq f < 1$, such that $nf$ integer
  \For{$i \gets 1$ to $l$}
    \State $b \getsr \Ber(p)$ \Comment insert $a_i$ with probability $p$ (and remove it otherwise)
    \If{$b$}
      \State $\chi \gets \chi \cup \{a_i\}$
    \Else
      \State $\chi \gets \chi - \{a_i\}$
    \EndIf
    \If{$|\chi| = n$} \Comment if buffer $\chi$ is full
      \State $\chi \getsr \mathrm{subsample}(\chi)$ \Comment select a random $nf$-subset of $\chi$
      \State $p \gets pf$
    \EndIf
  \EndFor
  \State \Return $\frac{|\chi|}{p}$ \Comment estimate cardinality of $A$
  \end{algorithmic}
\end{algorithm}%

When we look at the subsampling step of~\cref{alg:cvm}, our invariant~\cref{i:func_invariant} imposes the following condition on the subsampling operation.
It should be noted that the condition becomes apparent while establishing \cref{i:func_invariant} using similar but more general steps as described in Equations~\ref{eq:step_1_exp} and \ref{eq:step_2_exp}.%
\footnote{A step-by-step proof of the derivation is also available in the appendix of the formalization~\cite[Appendix~A]{CVM_Distinct_Elements-AFP}.}
\begin{equation}\label[ineq]{i:subsample_condition}
  \integral{\mathrm{subsample}(\chi)}{\prod_{s \in S} g(\indicat(s \in \tau))}{\tau} \leq \prod_{s \in S} \expect_{\Ber(f)} [g]
\end{equation}
for all non-negative functions $g$ and any $S \subseteq \chi$, where $f$ is the probability of retaining each element in the subsampling step of the algorithm.
Any subsampling step that satisfies this functional inequality can be used while still preserving~\cref{i:func_invariant} for the algorithm.

Motivated by this observation, our new variant is shown in \cref{alg:cvm_new}.
For the subsampling step, instead of keeping each element of $\chi$ with probability $\frac{1}{2}$, we pick a uniform random $nf$-subset of $\chi$, where $\frac{1}{2} \leq f < 1$ and $nf$ is an integer.
For example, it is possible to choose $f = \frac{n-1}{n}$, i.e., discarding a random element from $\chi$ in the subsampling step.
Since this new subsampling step always reduces the size of $\chi$, the variant is \emph{total} (never returns $\bot$).
The invariant-based approach allows us to show that the algorithm is probably-approximately correct and also \emph{unbiased}, i.e., the expectation of the result is exactly $|A|$.
These depend crucially on establishing~\cref{i:subsample_condition} for the new subsampler, for which we need a new concept.

\subsection{Background on Negative Dependence}
Some sets of random variables possess a property called \emph{negative association}, a generalization of independence.
The concept was introduced by Joag-Dev and Proschan~\cite{joagdev1983}, who showed that it has many useful closure properties compared to other previously introduced notions of negative dependence, such as negative correlation or negative orthant dependence. %TODO: perhaps citations for these two?
Importantly, standard Chernoff--Hoeffding type bounds still apply to negatively associated random variables~\cite[Prop. 7]{dubhashi1998}.
Negative association is defined as follows:
\begin{definition}
For a function defined on $n$-tuples $f: V^n \rightarrow W$, we will denote by $\mathrm{dep}(f)$ the set of coordinates the function depends on, i.e., $dep(f) \subseteq \{1,\ldots,n\}$ is minimal, such that $f(x) = f(y)$ for all $x, y \in V^n$ with $x_i = y_i$ for all $i \in dep(f)$.
\end{definition}

\begin{definition}[Negative Association]\label{def:neg_assoc}
A set of random variables $X_1,\dots,X_n: \Omega \rightarrow \mathbb R$ is negatively associated if, for all non-decreasing functions $f,g: \mathbb R^n \rightarrow \mathbb R$, which depend on disjoint sets of the variables, i.e., $\mathrm{dep}(f) \cap \mathrm{dep}(g) = \emptyset$, the following inequality holds:
\[
\expect [f(X_1,\ldots,X_n) g(X_1,\ldots,X_n)] \leq \expect [f(X_1,\ldots,X_n)] \expect [g(X_1,\ldots,X_n)] \textrm{.}
\]
\end{definition}

The following proposition summarizes some important properties of negatively associated sets of random variables.
\begin{proposition}[Summary of results for negatively associated random variables \cite{joagdev1983}]\label{pro:neg_dep_props}~
\begin{enumerate}
\item \label{it:neg_dep_props:mult_mono} If $X=(X_1,\ldots,X_n)$ are negatively associated then $\expect [f(X) g(X)] \leq \expect [f(X)] \expect [g(X)]$ for non-increasing functions $f,g$ with $\mathrm{dep}(f) \cap \mathrm{dep}(g) = \emptyset$.
\item If $X=(X_1,\ldots,X_n)$ are negatively associated, $Y=(Y_1,\ldots,Y_m)$ are negatively associated, and the pair of vector-valued random variables $X$ and $Y$ are independent, then the union $X_1,\dots,X_n,Y_1,\dots,Y_m$ is a set of negatively associated random variables.
\item If $X=(X_1,\ldots,X_n)$ are negatively associated and $f_1, \dots ,f_m : \mathbb R^n \rightarrow \mathbb R$ are all non-increasing or all non-decreasing functions, such that $\mathrm{dep}(f_i) \cap \mathrm{dep}(f_j) = \emptyset$ for $i \neq j$, then $f_1(X),\ldots,f_m(X)$ form a set of negatively dependent random variables of size $m$.
\item If $X_1,\dots,X_n$ are independent then $X_1,\dots,X_n$ are negatively associated.
\item A subset of a negatively associated set of random variables is again negatively associated.
\end{enumerate}
\end{proposition}

These properties illustrate the trade-off between negative association and independence.
For example, Property 3 would be true for independent random variables, even without the condition of monotonicity.
%Of course, on the other hand independence is a stronger property and fewer sets of random variables are independent.
To analyze our new subsampler, the following is an important lemma about negatively associated random variables.

\begin{lemma}\label{le:neg_assoc_prod}
Let $X_1,\dots,X_n$ be negatively associated and $f_1,\dots,f_n$ be all non-decreasing (or all non-increasing), non-negative functions, then
\[
  \expect \left[\prod_{i=1}^{n} f_i(X_i)\right] \leq \prod_{i=1}^{n} f_i(\expect [X_i]) \textrm{.}
\]
\end{lemma}
\begin{proof}
This follows from the definition of negative association (or Property~1 of \cref{pro:neg_dep_props}, if the $f_i$ are non-increasing) using induction.
\end{proof}

The case for non-decreasing functions of the above lemma is pointed out by Joag-Dev and Proschan~\cite[P.2]{joagdev1983}.
The reason for our interest in this lemma stems from the fact that indicator variables of random $m$-subsets are negatively associated.
This is a consequence of the fact that permutation distributions are negatively associated~\cite[Th. 2.11]{joagdev1983}.
Thus, for the new subsampling step in Line 9 of~\cref{alg:cvm_new}, we can derive using \cref{le:neg_assoc_prod}:
\begin{equation}\label{eq:subsample_with_n_subsets}
  \integral{\mathrm{subsample}(\chi)}{\prod_{s \in S} g(\indicat(s \in \tau))} {\tau} \leq
  \prod_{s \in S} \integral{\mathrm{subsample}(\chi)}{g(\indicat(s \in \tau))}{\tau} = \prod_{s \in S}  \expect_{\Ber(f)}[g] \textrm{.}
\end{equation}
for any non-negative $g$ and $S \subseteq \chi$.
Note that the domain of $g$ has two values, so it is either non-increasing or non-decreasing.
Also, if $S$ is a singleton, the inequality becomes an equality.
With this ingredient, we can conclude that our results about the original algorithm derived in the previous section also hold for our new variant (\cref{alg:cvm_new}).
% Note: After reading this section. It stops too abrubtly at the previous sentence, without establishing what we have actually shown.
