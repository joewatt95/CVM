\section{Introduction}
\label{sec:intro}

In 2022, Chakraborty, Vinodchandran, and Meel~\cite{chakraborty2022} published a marvelous streaming algorithm for the distinct elements problem which was very unexpected in the community~\cite{quanta}.
Indeed, Knuth later wrote a note on the algorithm~\cite{knuthnote}, pointing out its interesting properties and christening it the \emph{CVM} algorithm (which we use for the rest of this paper).
One striking property of the CVM algorithm is that, in contrast to every other known algorithm for the problem, it does not rely on hashing the stream elements.
Instead, the algorithm could theoretically be implemented in a setting where objects in the data stream only allow for equality comparisons.
Another property is its simplicity---which is why the authors called it ``an algorithm for the text book''---the algorithm is displayed in its entirety in~\cref{alg:cvm}.

\begin{algorithm}[h!]
	\caption{CVM algorithm for distinct elements estimation~\cite{chakraborty2022}.}\label{alg:cvm}
	\begin{algorithmic}[1]
  \Require Stream elements $a_1,\dots,a_l$, $0 < \varepsilon$, $0 < \delta < 1$.
  \Ensure A cardinality estimate $R$ for set $A = \{ a_1,\dots,a_l \}$ s.t. $\prob \left( |R - |A| | > \varepsilon |A| \right) \leq \delta$
  \State $\chi \gets \{\}, p \gets 1, n = \ceil*{\frac{12}{\varepsilon^2} \ln{(\frac{6l}{\delta})} }$
  \For{$i \gets 1$ to $l$}
    \State $b \getsr \Ber(p)$ \Comment insert $a_i$ with probability $p$ (and remove it otherwise)
    \If{$b$}
      \State $\chi \gets \chi \cup \{a_i\}$
    \Else
      \State $\chi \gets \chi - \{a_i\}$
    \EndIf
    \If{$|\chi| = n$} \Comment if buffer $\chi$ is full
      \State $\chi \getsr \mathrm{subsample}(\chi)$ \Comment discard elements of $\chi$ independently with prob. $\frac{1}{2}$
      \State $p \gets \frac{p}{2}$
    \EndIf
    \If{$|\chi| = n$}
      \Return $\bot$ \Comment fail if $\chi$ remains full
    \EndIf
  \EndFor
  \State \Return $\frac{|\chi|}{p}$ \Comment estimate cardinality of $A$
  \end{algorithmic}
\end{algorithm}

The pen-and-paper analysis of CVM~\cite{chakraborty2022,chakraborty2023} relies on a sequence of transformations of the algorithm.
The reason for these transformations is that standard methods for analyzing randomized algorithms, such as Chernoff--Hoeffding bounds, usually make statements about independent random variables.
However, for \cref{alg:cvm}, the state variables are far from being independent.\footnote{There is an incorrect claim in the initial published proof of CVM~\cite[Claim 6]{chakraborty2022} that the indicator functions for elements in $\chi$ are independent; a later version by the same authors~\cite{chakraborty2023} provides a correct proof.
The original error serves as a side motivation for this work.}
For example, in Line~3 the Bernoulli distribution is sampled for the value $p$, which itself depends on previous random operations; similarly, the subsampling step in Line~9 is only applied if the buffer $\chi$ is full, which also depends on previous random operations.
The aforementioned sequence of transformations by Chakraborty et al.~results in another randomized algorithm which can be analyzed using standard methods, and from which the desired results for the original algorithm can be deduced.
To our knowledge, it seems impossible to analyze \cref{alg:cvm} more directly using known textbook methods~\cite{alon2000,mitzenmacher2017,motwani1995}.

In this paper, we present a new technique for analyzing randomized algorithms which yields a direct and substantially more general proof of the CVM algorithm.
Our approach is very similar to how deterministic algorithms are verified using loop invariants.
The key difference is that our choice of ``loop invariant'' for the randomized streaming algorithm is a functional probabilistic inequality, namely, we consider invariants of the form:
\[
  \expect [ h ] \leq h(c)
\]
where $h$ is allowed to range over a class of functions, and the expectation is taken over the distribution of the state of the algorithm after consuming each stream element.
By first establishing such an invariant for \cref{alg:cvm}, we can then use it (via different choices of $h$) to establish error bounds for the algorithm.
%For the rest of this paper, we explain this technique, its mechanization, and show how it leads to a proof of the CVM algorithm.
We believe the new proof remains accessible at the undergraduate level, albeit with some exposure to mechanized theorem proving.

To show the generality of our technique, we introduce a new variant of the CVM algorithm, where the subsampling step in Line~9 of \cref{alg:cvm} selects a random $m$-subset of $\chi$ instead of independently discarding each element with some probability.
This variant has the benefit that it is \emph{total} (never returns $\bot$) because the second check in Line~11 becomes obsolete.
More interestingly, the resulting variant is \emph{unbiased}, i.e., the expected result is exactly the cardinality of the elements in the stream; this is a new property that neither the original CVM algorithm nor classic algorithms for the distinct elements problem possess.

The modified subsampling step leads to additional dependence for the elements in $\chi$ which cannot be readily removed using transformations following the original proof.
Instead, we verify the new variant with our probabilistic invariant-based approach, using results from the theory of negatively associated random variables~\cite{joagdev1983} to establish the desired functional invariant.
The concept of negative association is a generalization of independence; importantly, negatively associated variables observe closure properties and fulfill Chernoff--Hoeffding bounds similar to independent random variables.
It should be stressed that the theory of negatively association is orthogonal to our new technique, but its formalization is also a contribution of this work.

In summary, our main contributions are:
\begin{itemize}
\item Introduction of a new technique using functional probabilistic invariants to verify randomized algorithms inductively/recursively.
\item Verification of the original CVM algorithm using our new technique.
\item Presentation and verification of a new variant of CVM that is total and unbiased.
\item Formalization of a theory of negatively associated random variables used to analyze the new CVM variant.
\end{itemize}

We have also mechanized the transformation-based CVM proof by Chakraborty et al.~\cite{chakraborty2022,chakraborty2023}.
As a rough point of comparison, verification of the CVM algorithm using our new technique requires only \locnew~lines in Isabelle/HOL~\cite{nipkow2002}, while the original proof required \locold~lines.

The rest of this paper is organized as follows.
\cref{sec:background} provides background information on randomized algorithms, in particular on their semantics in Isabelle/HOL.
\cref{sec:invariants} introduces our new technique and explains how probabilistic loop invariants can be used to establish tail bounds for the original CVM algorithm.
\cref{sec:negdep} introduces the concept of negative association and our new total and unbiased variant of the CVM algorithm.
\cref{sec:formalization} presents the formalization of both variants of the algorithm, and \cref{sec:formalization_neg_dep} describes our new formalized library on negatively associated random variables.
\cref{sec:transformation_based_proof} discusses some challenges faced in our alternative verification of CVM using the transformation-based proof by Chakraborty et al.
The final sections conclude with an overview of related work and a summary of our results.

The supplementary material contains:
\begin{itemize}
\item formalization of the CVM algorithm, both the original version (\cref{alg:cvm}) and our new version (\cref{alg:cvm_new}) using probabilistic functional invariants;
\item formalization of a library for negative association; and %in the directory Negative{\textunderscore}Association, which we intend to submit to the AFP as a separate contribution.
\item formalization of the CVM algorithm following the proof by Chakraborty et al.~\cite{chakraborty2022,chakraborty2023}.
\end{itemize}

% TODO: move it into the discussion of the variant, I think it is better once the algorithm is shown
%This is because indicator functions of $n$-subsets form negatively associated random variables (RV), even though they are not independent, with which we can modify the subsampling step to the form we described above.

%TODO: I think this should be later, we can give a forward reference
%% The algorithm's state is a buffer $\chi$  (initially empty) and a fraction $p > 0$ (initially set to $1$).
%% The buffer contains a subset of the elements of the stream encountered so far, with maximal size $n$.
%% The size is chosen according to the desired accuracy parameters $\varepsilon$, $\delta$, and the stream size $l$.
%% The algorithm iterates over stream elements, adding each one to the buffer with probability $p$ or conversely -- if the current stream element is already in the buffer -- removing them with probability $(1-p)$.
%% If the buffer gets too large, approximately half of the elements are removed by discarding each element in $\chi$ independently with probability $\frac{1}{2}$; then, $p$ is adjusted to reflect the fact that the buffer now contains each element with probability $p_\text{new} = \frac{p_\text{old}}{2}$.
%% After processing the stream, the algorithm returns $\frac{|\chi|}{p}$ as an approximation of the number of distinct elements in the stream.

%% %TODO: a bit too long in details, we should trim this and move to later section
%% \subparagraph*{The road not taken:}
%% One of the difficult aspects of the proof is that it relies on an eager-lazy coin flip conversion.
%% To understand that, we should note that none of the observable random variables, such as the presence of a stream element in the buffer or conditions on the value of $p$, are independent of the other state variables, which makes the algorithm hard to analyze and makes the application of standard techniques from probability theory, such as Hoeffding's theorem impossible.
%% The authors resolved that problem by a simulation argument---they show that~\cref{alg:cvm} behaves stochastically identically to a different algorithm, which makes the relevant coin flips in a different order.
%% That modified algorithm performs a column of coin flips for each stream element.
%% An element is kept in the buffer if the first $k=\log_2(p-1)$ rows of the column are heads.
%% At each sub-sampling step, when p is divided by two, i.e., if k increases by one.
%% The algorithm examines the newly activated $k$-th row of the previous sequence elements to decide whether the element should be kept in the buffer.
%% This preserves the invariant that the buffer consists of exactly those sequence elements whose associated coin flip column starts with $\log_2(p-1)$ heads.
%% Of course, the new algorithm is not practical for actual implementation, but one can verify its correctness using standard Chernoff bounds, and on the other hand, it is possible to show that its behavior is equivalent to Algorithm 1.
%% To summarize, while the algorithm is marvelous, the proof was still very technical.
%% The simulation argument, in particular, is not so elegant to formalize.

%% \subparagraph*{A more direct proof:}
%% We set out to try to find a more direct proof, which also eases the formalization effort.
%% For the following discussion, we will analyze Algorithm 1 with line 8 removed, i.e., the algorithm does not output $\bot$, nor performs a second check of $|\chi|=n$.
%% Note that this happens if the very improbable event where none of the elements in X are removed during a subsampling step, which happens with probability at most $2^{-n/2}$.
%% Overall, the probability of it happening during the course of the algorithm is at most $\frac{\delta}{2}$. (Note that removing line 8 does not affect the correctness of the algorithm, but it loses its space consumption bound.)
%% It is easy to see that any probability established about the algorithm missing line 8 will be true for the original algorithm with a possible correction by, at most,  $\frac{\delta}{2}$.
%% We will remember and correct this at the end of this section.

%% Let us consider an imaginary situation where, somehow, $p$ is fixed at some point in the algorithm.
%% For example, we could imagine a final sub-sampling loop, which is run until a fixed $p$ is reached.
%% Then, the indicator random variables representing the presence of a stream element TODO
