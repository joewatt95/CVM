\section{Introduction}
\label{sec:intro}

In 2022, Chakraborty, Vinodchandran, and Meel~\cite{chakraborty2022} published a marvelous streaming algorithm for the distinct elements problem which was very unexpected in the community~\cite{quanta}.
Indeed, Knuth later wrote a note on the algorithm~\cite{knuthnote}, pointing out its interesting properties and christening it the \emph{CVM} algorithm (which we use for the rest of this paper).
One striking property of the CVM algorithm is that, in contrast to every other known algorithm for the problem, it does not rely on hashing the stream elements.
Instead, the algorithm could theoretically be implemented in a setting where objects in the data stream only allow for equality comparisons.

Another property of CVM is its simplicity, both in terms of its description---the algorithm is listed in its entirety in~\cref{alg:cvm}---and its pen-and-paper correctness proof, which only requires undergraduate-level exposure to randomized algorithms.
This is why its authors called it ``an algorithm for the text book''.
The motivation of this work is to explore whether CVM's simplicity transfers into a mechanization within the Isabelle proof assistant~\cite{nipkow2002}.

\begin{algorithm}[h!]
	\caption{CVM algorithm for distinct elements estimation~\cite{chakraborty2022}.}\label{alg:cvm}
	\begin{algorithmic}[1]
  \Require Stream elements $a_1,\dots,a_l$, $0 < \varepsilon$, $0 < \delta < 1$.
  \Ensure A cardinality estimate $R$ for set $A = \{ a_1,\dots,a_l \}$ s.t. $\prob \left( |R - |A| | > \varepsilon |A| \right) \leq \delta$
  \State $\chi \gets \{\}, p \gets 1, n = \ceil*{\frac{12}{\varepsilon^2} \ln{(\frac{6l}{\delta})} }$
  \For{$i \gets 1$ to $l$}
    \State $b \getsr \Ber(p)$ \Comment insert $a_i$ with probability $p$ (and remove it otherwise)
    \If{$b$}
      \State $\chi \gets \chi \cup \{a_i\}$
    \Else
      \State $\chi \gets \chi - \{a_i\}$
    \EndIf
    \If{$|\chi| = n$}
      \State $\chi \getsr \mathrm{subsample}(\chi)$ \Comment discard elements of $\chi$ independently with prob. $\frac{1}{2}$
      \State $p \gets \frac{p}{2}$
    \EndIf
    \If{$|\chi| = n$}
      \Return $\bot$ \Comment fail if $\chi$ remains too large
    \EndIf
  \EndFor
  \State \Return $\frac{|\chi|}{p}$ \Comment estimate cardinality of $A$
  \end{algorithmic}
\end{algorithm}

Curiously, our formalization takes a substantially different route compared to the original proof~\cite{chakraborty2022,DBLP:journals/corr/abs-2301-10191}.
Briefly, the pen-and-paper analysis for CVM relies on a sequence of transformations of the algorithm.
The reason for these transformations is that standard methods for analyzing randomized algorithms, such as Chernoff--Hoeffding bounds, usually make statements about independent random variables.
Yet, for~\cref{alg:cvm} the random operations are far from being independent.\footnote{This was an incorrect claim in the initial published proof of CVM~\cite[Claim 6]{chakraborty2022}; a later version by the same authors~\cite{DBLP:journals/corr/abs-2301-10191} provides a correct proof.
The original error serves as a side motivation for this work.}
For example, in line 3 the Bernoulli distribution is sampled for a value $p$, which itself depends on previous random operations; similarly, the subsampling step in line 9 is only applied if the buffer is full, which also depends on previous random operations.
The aforementioned sequence of transformations bounds the error of~\cref{alg:cvm} in terms of another randomized algorithm with more desirable independence properties.
Indeed, it seems impossible to analyze~\cref{alg:cvm} directly using known methods.

This is the point where our new analysis technique comes in---it is very similar to how deterministic algorithms are verified using a loop invariant.
The key difference is that our choice of ``loop invariant'' for the randomized streaming algorithm is a functional probabilistic inequality, namely, we consider invariants of the form:
\[
  \expect [ h ] \leq h(c)
\]
where $h$ is allowed to range over a class of functions, and the expectation is taken over the distribution of the state of the algorithm after consuming each stream element.
By first establishing such an invariant for~\cref{alg:cvm}, we can then use it (via different choices of $h$) to establish error bounds for the algorithm.
For the rest of this paper, we explain this technique, its mechanization, and show how it leads to a simple but substantially more general proof of the CVM algorithm.
We believe the new proof remains accessible at the undergraduate level, albeit with some exposure to interactive theorem proving.

The main contributions of this work are:
\begin{itemize}
\item Introduction of a new technique using functional probabilistic invariants to verify randomized algorithms inductively/recursively.
\item Verification of the original CVM algorithm using our new technique.
\item Presentation and verification of a new variant of CVM that is total and unbiased.
\item Formalization of a theory of negatively associated random variables used to analyze the new CVM variant.
\end{itemize}

Verification of CVM using our new technique requires only 1044 lines in Isabelle~\cite{nipkow2002}, while the original proof required \todo{x} lines.
We detail some of the challenges faced when mechanizing the reduction arguments used by Chakraborty et al.~\cite{chakraborty2022,DBLP:journals/corr/abs-2301-10191} in Section~\todo{x}.

The formalized CVM variant showcases the utility of our new proof technique.
In this variant, the subsampling step in line 9 of \cref{alg:cvm} selects a random $m$-sized subset of $\chi$, instead of independently deciding for each element, whether to keep it.
This variant has the benefit that it is \emph{total} (never returns $\bot$) because the second check in line 11 becomes obsolete.
More interestingly, the resulting variant is \emph{unbiased}, i.e., the expected result is exactly the cardinality of the elements in the stream; this is a new property, that neither the original CVM algorithm nor classic algorithms for the distinct elements problem possess.

To analyze the new variant, we use results from the theory of negatively dependent random variables to establish the desired functional invariant.
The concept of negative association is a generalization of independence; negatively associated variables observe closure properties and fulfill Chernoff--Hoeffding bounds similar to independent random variables.
It should be stressed that the theory of negatively associated RVs is orthogonal to our new technique, but the formalization of such a theory is also a contribution of this work.

% TODO: move it into the discussion of the variant, I think it is better once the algorithm is shown
%This is because indicator functions of $n$-subsets form negatively associated random variables (RV), even though they are not independent, with which we can modify the subsampling step to the form we described above.

% TODO: move it into explanation of the technique
%Note that we are able to establish tail bounds using these invariants, which has, so far, not been possible using a simple loop invariant for randomized algorithms.
%The loop invariants are established, essentially, using the key property of the Giry monad:
%\[
%  \expect_{m \isa{\isasymbind} f} [h] = \int_m \expect_{f (x)} [h] \, d x \textrm{.}
%\]
%Here $m \isa{\isasymbind} f$ denotes distribution of a randomized algorithm, which represents the sequential composition of $m$ with $f$.

%TODO: I think this should be later, we can give a forward reference
%% The algorithm's state is a buffer $\chi$  (initially empty) and a fraction $p > 0$ (initially set to $1$).
%% The buffer contains a subset of the elements of the stream encountered so far, with maximal size $n$.
%% The size is chosen according to the desired accuracy parameters $\varepsilon$, $\delta$, and the stream size $l$.
%% The algorithm iterates over stream elements, adding each one to the buffer with probability $p$ or conversely -- if the current stream element is already in the buffer -- removing them with probability $(1-p)$.
%% If the buffer gets too large, approximately half of the elements are removed by discarding each element in $\chi$ independently with probability $\frac{1}{2}$; then, $p$ is adjusted to reflect the fact that the buffer now contains each element with probability $p_\text{new} = \frac{p_\text{old}}{2}$.
%% After processing the stream, the algorithm returns $\frac{|\chi|}{p}$ as an approximation of the number of distinct elements in the stream.
%% This output is probably-approximately correct, i.e., the probability that the relative error of $\frac{|\chi|}{p}$ exceeds $\varepsilon$ is at most $\delta$.
%The output of CVM is \emph{probably-approximately correct}, i.e., the probability that the relative error of its output exceeds $\varepsilon$ is at most $\delta$.
%Moreover, let us assume the space needed to store each element in the stream is $b$ bits, then the algorithm requires only $\bigo(\varepsilon^{-2} b \ln(\delta^{-1} l))$ bits of mutable state---far less than storing each stream element deterministically.%
%\footnote{The optimal randomized algorithm requires $\bigo( \varepsilon^{-2} \ln \delta + b)$ bits, but it requires more advanced algorithmic techniques. It would not be possible to present using such elementary steps as used in \cref{alg:cvm}, and involves computations in finite fields and random walks in expander graphs~\cite{blasiok2020, karayel2023}.}
%

\todo{Classic summary of sections.}


%% We set out to formalize the proof as described in the literature~\cite{chakraborty2022}, hoping to show that it would also be an easy formalization exercise.

%% %TODO: a bit too long in details, we should trim this and move to later section
%% \subparagraph*{The road not taken:}
%% One of the difficult aspects of the proof is that it relies on an eager-lazy coin flip conversion.
%% To understand that, we should note that none of the observable random variables, such as the presence of a stream element in the buffer or conditions on the value of $p$, are independent of the other state variables, which makes the algorithm hard to analyze and makes the application of standard techniques from probability theory, such as Hoeffding's theorem impossible.
%% The authors resolved that problem by a simulation argument---they show that~\cref{alg:cvm} behaves stochastically identically to a different algorithm, which makes the relevant coin flips in a different order.
%% That modified algorithm performs a column of coin flips for each stream element.
%% An element is kept in the buffer if the first $k=\log_2(p-1)$ rows of the column are heads.
%% At each sub-sampling step, when p is divided by two, i.e., if k increases by one.
%% The algorithm examines the newly activated $k$-th row of the previous sequence elements to decide whether the element should be kept in the buffer.
%% This preserves the invariant that the buffer consists of exactly those sequence elements whose associated coin flip column starts with $\log_2(p-1)$ heads.
%% Of course, the new algorithm is not practical for actual implementation, but one can verify its correctness using standard Chernoff bounds, and on the other hand, it is possible to show that its behavior is equivalent to Algorithm 1.
%% To summarize, while the algorithm is marvelous, the proof was still very technical.
%% The simulation argument, in particular, is not so elegant to formalize.

%% \subparagraph*{A more direct proof:}
%% We set out to try to find a more direct proof, which also eases the formalization effort.
%% For the following discussion, we will analyze Algorithm 1 with line 8 removed, i.e., the algorithm does not output $\bot$, nor performs a second check of $|\chi|=n$.
%% Note that this happens if the very improbable event where none of the elements in X are removed during a subsampling step, which happens with probability at most $2^{-n/2}$.
%% Overall, the probability of it happening during the course of the algorithm is at most $\frac{\delta}{2}$. (Note that removing line 8 does not affect the correctness of the algorithm, but it loses its space consumption bound.)
%% It is easy to see that any probability established about the algorithm missing line 8 will be true for the original algorithm with a possible correction by, at most,  $\frac{\delta}{2}$.
%% We will remember and correct this at the end of this section.

%% Let us consider an imaginary situation where, somehow, $p$ is fixed at some point in the algorithm.
%% For example, we could imagine a final sub-sampling loop, which is run until a fixed $p$ is reached.
%% Then, the indicator random variables representing the presence of a stream element TODO
