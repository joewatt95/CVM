\section{Transformation-based Proof}\label{sec:transformation_based_proof}
In this section we describe the interesting parts of the original transformation-based proof by Chakraborty et al.~\cite{chakraborty2022}.
This highlights in part, why it requires a lot of work to formalize, and why we were motivated to develop our new approach using probabilistic invariants.

As we mentioned in the introduction, the main difficulty is the fact that \cref{alg:cvm}'s state variables are not independent.
The trick Chakraborty et al. use to circumvent the problem is by modifying the algorithm, in a manner that obviously preserves its distribution, such that the resulting algorithm's state can be described in terms of independent coin flips.

Let us consider a state, where $k$ subsampling steps have been performed, i.e., $p = 2^{-k}$. 
Then the algorithm would normally perform a coin flip with probability $p$.
In the modified algorithm, we perform a fixed number of coin flips for each sequence element at the beginning.
The element is put into the sample, whenever the first $k$ coin flips associated with the sequence element are $1$.
Note that this happens exactly with probability $2^{-k}$, which means the behaviour of the algorithm is unchanged.

Moreover, during the subsampling operation those elements are kept, whose $k+1$-th associated coin flip is $1$.
This again preserves the behaviour, that each element is discared independently with probability $1/2$.
The operation $p \mapsto \frac{p}{2}$ is replaced with $k \mapsto k+1$.
For this modified algorithm, it is easy to see that the set of elements in any state are exactly, those stream elements for which the first $k$ entries of their associated coin flips are $1$.

In \cref{alg:cvm_simul} we describe the new algorithm, that behaves exactly as the original one (\cref{alg:cvm}), but performs indepenent coin flips.
Note that the function $\mathrm{last{\textunderscore}index}$ returns the index of the last-occurence of an element in the sequence, before the current loop iteration.
It should be noted that the algorithm keeps track of the number of subsampling iterations $k$, instead of the value $p = 2^{-k}$ as the original algorithm does.
%Like, we did in \cref{sec:invariants}, we ignore the second check, whether the subsampling operation succeeded.
%As we explained there, the total variational difference between these two variants is $\frac{\delta}{2}$.
%
\begin{algorithm}[h!]
	\caption{Modified CVM algorithm with independent coin flips.}\label{alg:cvm_simul}
	\begin{algorithmic}[1]
  \Require Stream elements $a_1,\dots,a_l$, $0 < \varepsilon$, $0 < \delta < 1$.
  \State $\chi \gets \{\}, k \gets 0, n = \ceil*{\frac{12}{\varepsilon^2} \ln{(\frac{6l}{\delta})} }$
  \State $b[i,j] \getsr \Ber(1/2)$ for $i,j \in \{1,\cdots,l\}$ \Comment perform $l^2$ unbiased independent coin flips
  \For{$i \gets 1$ to $l$}
    \If{$b[i,1]=b[i,2]=\cdots=b[i,k]=1$}
      \State $\chi \gets \chi \cup \{a_i\}$
    \Else
      \State $\chi \gets \chi - \{a_i\}$
    \EndIf
    \If{$|\chi| = n$}
      \State $\chi \gets \{a \in \chi \;|\; b[\mathrm{last{\textunderscore}index}(a),k+1] = 1\}$
      \State $k \gets k+1$
    \EndIf
    \If{$|\chi| = n$}
      \State \Return $\bot$
    \EndIf
  \EndFor
  \State \Return $2^k |\chi|$ \Comment estimate cardinality of $A$
  \end{algorithmic}
\end{algorithm}
To roughly explain, how tail bounds can be derived for this new variant:
It is possible to union bound the probability that the estimate exceeds the desired interval, with the probability of the event in conjunction with a specific value of $k$, which can be bounded by the probability that the number of stream elements with whose associated coin flips start with $k$ ones, is outside of $2^{-k} |A| (1 \pm \varepsilon)$.
This is explained in more detail by Chakraborty et al.~\cite{chakraborty2022}.

One of the key questions is, how to formalize the transformation from \cref{alg:cvm} to this new variant.
What we discovered is that it best to solve the problem backwards, i.e., we start with the modified algorithm, which performs all the coin-flips in advance --- eagerly --- and convert it to the version, that performs the coin flips --- lazily --- at the point they are needed.

We can push down the coin flips through the expression tree.
To explain how this works.
Let us first define the \emph{sampling} function, i.e., let $f$ be a function that depends on coin flips, more precisely $f$ takes as argument a vector of coin flips indexed by $I$, then we can express the distribution of $f$ with respect to indepenent unbiased coin flips as.
\begin{isabelle_cm}
  sample\ f\ \isacharequal\ map{\isacharunderscore}pmf\ f\ {\isacharparenleft}prod{\isacharunderscore}pmf\ I\ {\isacharparenleft}\isasymlambda\isacharunderscore\isachardot\ bernoulli{\isacharunderscore}pmf \isacharparenleft\isadigit{1}/\isadigit{2}\isacharparenright\isacharparenright\isacharparenright
\end{isabelle_cm}
%For example, we could use our modified algorithm as $f$, with the index set $I = \{0, \ldots, l-1\} \times \{0, \ldots, l-1\}$.
The interesting fact is that we can distribute the sampling operation over composition, e.g.:
\begin{isabelle_cm}
  sample\ \isacharparenleft\isasymlambda\isasymomega\isachardot\ f\ \isasymomega\ \isasymcirc\ g\ \isasymomega{\isacharparenright}\ \isacharequal\ sample\ g\ \isasymbind\ \isacharparenleft{\isasymlambda}x\isachardot\ sample\ \isacharparenleft\isasymlambda\isasymomega\isachardot\ f\ \isasymomega\isachardot\ x\isacharparenright\isacharparenright
\end{isabelle_cm}
if $f$ and $g$ depend on disjoint subsets of the coin flips, i.e., if $\mathrm{dep}(f) \cap \mathrm{dep}(g) = \emptyset$.

By recursively applying the rule, we end up with elementary lookup operations, e.g., \isa{sample\ \isacharparenleft\isasymlambda\isasymomega\isachardot\ \isasymomega\ i\isacharparenright}, for which it is easy to see that it is just a coin flip, i.e., equal to \isa{bernoulli{\isacharunderscore}pmf\ \isacharparenleft\isadigit{1}/\isadigit{2}\isacharparenright}.
Note that, we arrive at the orignal version of the algorithm, that performs the coin flips lazily.

A detail, that we simplified here is that the split of the index sets, e.g., which coin flips $f$ depends on and which coin flips $g$ depends on, maybe dynamic.
This means the split may depend on the return value of $f$.
For example, when the algorithm increases the subsampling counter $k$, it will have read the corresponding row of coin-flips.
This means, we have a situation, where the previous loop iteration communicates to the next loop iteration, which coin flips it depends on using the state.
And the next loop iteration, will indeed only read coin flips that were not read by the previous iteration.
%For the column index of the coin flips, this is based on the stream index, but for the row index of the coin flips, it is based on the state variable $k$.
%% It is possible to express the above in a point-free manner.
%% For that, we need to introduce the reader monad.
%% This is a deterministic monad, which provides a read operation to a global value, such as our set of coin flips.
%% (In Haskell, the reader monad is commonly used to provide a fixed enviroment to algorithms, such as global configuration options, paths or command lines parameters~\cite{jones1995}.)

%% \begin{isabelle_cm}
%% \isacommand{datatype}\ \isacharparenleft{\isacharprime}c\isacharcomma\ {\isacharprime}a\isacharparenright\ reader{\isacharunderscore}monad\ =\ Reader\ {\isacharparenleft}run{\isacharunderscore}reader\isacharcolon\ \isacartoucheopen{\isacharprime}c\ \isasymRightarrow\ {\isacharprime}a\isacartoucheclose\isacharparenright)\isanewline
%% \isanewline
%% \isacommand{definition}\ bind{\isacharunderscore}rd\ \isacommand{where}\ {\isacartoucheopen}bind{\isacharunderscore}rd\ m\ f\ \isacharequal\ Reader\ \isacharparenleft{\isasymlambda}r\isachardot\ run{\isacharunderscore}reader\ {\isacharparenleft}f\ {\isacharparenleft}run{\isacharunderscore}reader\ m\ r\isacharparenright\isacharparenright\ r\isacharparenright\isacartoucheclose\isanewline
%% \isacommand{definition}\ return{\isacharunderscore}rd\ \isacommand{where}\ {\isacartoucheopen}return{\isacharunderscore}rd\ x\ \isacharequal\ Reader\ \isacharparenleft{\isasymlambda}{\isacharunderscore}\isachardot\ x\isacharparenright\isacartoucheclose\isanewline
%% \isacommand{definition}\ get{\isacharunderscore}rd\ \isacommand{where}\ {\isacartoucheopen}get{\isacharunderscore}rd\ x\ \isacharequal\ Reader\ id\isacartoucheclose
%% \end{isabelle_cm}
%% With the definition above, we can define our sampling function with respect to the reader monad, where the environment is the entire set of coin flips, which is now a monad morphism (from the reader monad to the Giry monad):
%% \begin{isabelle_cm}
%%   sample{\isacharunderscore}rd\ \isacharequal\ sample\ \isasymcirc\ run{\isacharunderscore}reader
%% \end{isabelle_cm}

