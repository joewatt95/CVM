\section{Transformation-Based Proof}\label{sec:transformation_based_proof}
Here, we describe the transformation-based proof by Chakraborty et al.~\cite{chakraborty2023}, focusing on the challenging parts in its formalization.
An overview of the proof is shown in~\cref{fig:proof_overview}, which highlights, in part, why the transformation-based approach required more work to formalize, and why we developed our new approach using probabilistic invariants.
\begin{note}
The transformation-based formalization of the CVM algorthm is included in the supplementary material~\cite{CVM_Transforms-Github}.
To formalize probabilistic transformations (relating two distributions), we built on an existing relational program logic in Isabelle/HOL~\cite{lochbihler2016}.
The formalization took \locold~lines which is considerably longer than the proof using our invariant-based technique (\locnew~lines).
\lipicsEnd\end{note}

As mentioned in~\cref{sec:intro}, the main difficulty in directly analyzing \cref{alg:cvm} is the lack of independence in its state variables.
The technique Chakraborty et al.\ use to circumvent this issue is by progressively modifying the algorithm (\circled{A}--\circled{D} in~\cref{fig:proof_overview}), in a manner that obviously bounds (or preserves) its distribution, and such that the final algorithm \circled{D} can be described using a simple random process with independent coin flips.

For interested readers, \circled{A} corresponds to~\cite[Algorithm 1]{chakraborty2023}, \circled{B} is~\cite[Algorithm 2]{chakraborty2023}, and \circled{D} is~\cite[Algorithm 3]{chakraborty2023}.
Whereas Chakraborty et al.\ move directly from \circled{B} to \circled{D} with an informal argument, \circled{C} is a transformation we added in the formalization to bridge this gap.

\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2.7cm, auto]

    % Define nodes
    \node (A) [draw, rectangle, minimum width=2.1cm, minimum height=1.2cm, align=center] {\circled{A} CVM \\ (Alg.~\ref{alg:cvm})};
    \node (B) [draw, rectangle, minimum width=2.1cm, minimum height=1.2cm, align=center, right of=A] {\circled{B} CVM w/o\\ Line 11};
    \node (C) [draw, rectangle, minimum width=2.1cm, minimum height=1.2cm, align=center, right of=B] {\circled{C} Eager Ver.\\(Alg.~\ref{alg:cvm_simul})};
    \node (D) [draw, rectangle, minimum width=2.1cm, minimum height=1.2cm, align=center, right of=C] {\circled{D} Random \\ Process};
    \node (E) [draw, rectangle, minimum width=2.1cm, minimum height=1.2cm, align=center, right of=D] {Chernoff \\ Bounds};

    \node (C') [draw, rectangle, minimum width=2.1cm, minimum height=1.2cm, align=center, below of=B, node distance = 2.0cm] {Functional\\Invariant};

    \node (pac) [draw, rectangle, minimum width=2.1cm, minimum height=1.2cm, align=center, below of=E, node distance = 2.0cm] {CVM PAC\\ Guarantee};

    \draw[thick] ($(A.north west) + (-0.3,0.2)$) rectangle ($(E.south east) + (0.3,-0.6)$);
    \node[anchor=south] at ($(D.south) + (0.0,-0.6)$) {Transformation-based Proof};

    \draw[dashed,thick] ($(A.north west) + (-0.5,0.4)$) rectangle ($(C'.south east) + (0.2,-0.2)$);
    \node[anchor=south,align=center] at ($(C'.west) + (-1.5,-0.45)$) {Probabilistic\\Invariant Proof};


    % Draw arrows
    \draw[->] (A) -- (B);
    \draw[->] (B) -- (C);
    \draw[->] (B) -- (C');
    \draw[->] (C) -- (D);
    \draw[->] (D) -- (E);
    \draw[->] (E) -- (pac);
    \draw[->] (C') -- (pac);

\end{tikzpicture}
\end{center}
\caption{An overview of the two formalized proof approaches for the CVM algorithm.}\label{fig:proof_overview}
\end{figure}

\subsection{A Bridging Transformation}
\begin{algorithm}[t]
	\caption{Modified CVM algorithm with independent coin flips. The function last{\textunderscore}index returns the index of the last occurrence of an element in the sequence, before the current iteration.}\label{alg:cvm_simul}
	\begin{algorithmic}[1]
  \Require Stream elements $a_1,\dots,a_l$, $0 < \varepsilon$, $0 < \delta < 1$.
  \Ensure A cardinality estimate $R$ for set $A = \{ a_1,\dots,a_l \}$ such that $\prob \left( |R - |A| | > \varepsilon |A| \right) \leq \delta$
  \State $\chi \gets \{\}, k \gets 0, n = \ceil*{\frac{12}{\varepsilon^2} \ln{(\frac{6l}{\delta})} }$
  \State $b[i,j] \getsr \Ber(1/2)$ for $i,j \in \{1,\cdots,l\}$ \Comment perform $l^2$ unbiased independent coin flips
  \For{$i \gets 1$ to $l$}
    \If{$b[i,1]=b[i,2]=\cdots=b[i,k]=1$} \Comment insert $a_i$ if first $k$ flips are $1$s.
      \State $\chi \gets \chi \cup \{a_i\}$
    \Else
      \State $\chi \gets \chi - \{a_i\}$
    \EndIf
    \If{$|\chi| = n$} \Comment if buffer $\chi$ is full
      \State $\chi \gets \{a \in \chi \;|\; b[\textrm{last{\textunderscore}index}(a),k+1] = 1\}$ \Comment keep elems.~whose $k+1$-th flip is $1$
      \State $k \gets k+1$
    \EndIf
    %\If{$|\chi| = n$}
    %  \State \Return $\bot$
    %\EndIf
  \EndFor
  \State \Return $2^k |\chi|$ \Comment estimate cardinality of $A$
  \end{algorithmic}
\end{algorithm}

Let us consider algorithm \circled{B} in a state where $k$ subsampling steps have been performed, i.e., $p = 2^{-k}$.
The algorithm would perform a coin flip lazily with probability $p$ when it encounters the next stream element.
The transformation \circled{C} is shown in \cref{alg:cvm_simul}, and we prove that it computes precisely the same distribution as \circled{B}.
In \circled{C}, we eagerly perform a fixed number of coin flips for each sequence element at the beginning.
Now, each element is put into the state $\chi$, whenever the first $k$ coin flips associated with the sequence element are all $1$s.
This happens exactly with probability $2^{-k}$, which means the behaviour of the algorithm is unchanged from \circled{B}.
Similarly, in the subsampling operation, only those elements whose $k+1$-th associated coin flip is $1$ are kept; the operation $p \mapsto \frac{p}{2}$ is replaced with $k \mapsto k+1$.
This again preserves the behaviour of \circled{B} that each element is discarded independently with probability $1/2$.

It is easy to show for \circled{C} that the coin flips are independent, and that the set of elements in $\chi$ in any state are exactly those stream elements for which the first $k$ entries of their associated coin flips are $1$.
The final random process $\circled{D}$ directly computes the final set of elements in $\chi$ after the stream, taking $K$ as a fixed parameter; one relates \circled{C} to \circled{D} by:
\[ \prob_{\circled{C}}(k = K \land \chi = X)  \leq \prob_{\circled{D}_K}(\chi = X)\]
for fixed values of $K$ and $X$.
To see how tail bounds can be derived from this inequality, let us first consider the failure event where the algorithm \circled{C}'s estimate exceeds the desired estimation interval and it ends with some fixed value $k = K$.
Using \circled{D}, this can be bounded using a Chernoff bound for the probability that the number of stream elements whose associated coin flips start with $K$ $1$s is outside $2^{-K} |A| (1 \pm \varepsilon)$.
Now, we can take a union bound over all the possible values $K$ to establish a global bound for the failure event in \circled{C}.
This is explained in more detail by Chakraborty et al.~\cite{chakraborty2023}.

%Like, we did in \cref{sec:invariants}, we ignore the second check, whether the subsampling operation succeeded.
%As we explained there, the total variational difference between these two variants is $\frac{\delta}{2}$.
%
\subsection{Eager to Lazy Coin Flips}
A remaining question is how to formalize the transformation from \circled{B} to \circled{C}.
Our insight is that it is best to solve the problem backwards, i.e., we start with the modified algorithm \circled{C}, which performs all the coin flips in advance \emph{eagerly} and convert it back to \circled{B} which implicitly performs the coin flips \emph{lazily} at the point they are needed.

The main idea is to automatically push down the coin flips through the expression tree of \cref{alg:cvm_simul}.
To explain how this works, let us first define the \emph{sampling} function, i.e., let $f$ be a function that takes as argument a vector of coin flips indexed by $I$, then we can express the distribution of $f$ with respect to independent unbiased coin flips as:
\begin{isabelle_cm}
  sample\ f\ \isacharequal\ map{\isacharunderscore}pmf\ f\ {\isacharparenleft}prod{\isacharunderscore}pmf\ I\ {\isacharparenleft}\isasymlambda\isacharunderscore\isachardot\ bernoulli{\isacharunderscore}pmf \isacharparenleft\isadigit{1}/\isadigit{2}\isacharparenright\isacharparenright\isacharparenright
\end{isabelle_cm}
%For example, we could use our modified algorithm as $f$, with the index set $I = \{0, \ldots, l-1\} \times \{0, \ldots, l-1\}$.
The interesting fact is that we can distribute the sampling operation over composition:
\begin{observation}\label{o:sample_distrib} Let $f,g$ be functions consuming a set of coin flips (indexed by $I$), where $g$ also consumes the output of $f$, such that,
$f$ depends only on the coin flips indexed by $J \subseteq I$ and $g$ depends on the complement $I - J$, then:
\begin{isabelle_cm}
  sample\ \isacharparenleft\isasymlambda\isasymomega\isachardot\ g\ \isasymomega\ \isasymcirc\ f\ \isasymomega{\isacharparenright}\ \isacharequal\ sample\ f\ \isasymbind\ \isacharparenleft{\isasymlambda}x\isachardot\ sample\ \isacharparenleft\isasymlambda\isasymomega\isachardot\ g\ \isasymomega\ x\isacharparenright\isacharparenright
\end{isabelle_cm}
\end{observation}
By recursively applying the observation, we end up with elementary lookup operations, e.g., \isa{sample\ \isacharparenleft\isasymlambda\isasymomega\isachardot\ \isasymomega\ i\isacharparenright}, for which it is easy to see that it is just a coin flip, i.e., equal to \isa{bernoulli{\isacharunderscore}pmf\ \isacharparenleft\isadigit{1}/\isadigit{2}\isacharparenright}.
This lets us readily transform \circled{C} to \circled{B} and prove their distributions equivalent.

A detail that we have simplified here is that the split of the index sets, e.g., which coin flips $f$ depends on and which coin flips $g$ depends on, may be dynamic.
For example, when the algorithm increases the subsampling counter $k$, it will have read the corresponding row of coin flips.
This means we have a situation where the previous loop iteration communicates to the next loop iteration which coin flips it depends on using the state; and the next loop iteration will indeed only read coin flips that were not read by the previous iteration.
%For the column index of the coin flips, this is based on the stream index, but for the row index of the coin flips, it is based on the state variable $k$.

To handle these situations we generalized \cref{o:sample_distrib} to allow for the case where the index $J$ splitting the set of coin flips $f$ and $g$ depend on, may itself depend on the result of $f$.
%It is possible to express the above in a point-free manner using the reader monad.
%This is a deterministic monad, which provides a read operation to a global value, such as our set of coin flips.
%(In Haskell, the reader monad is commonly used to provide a fixed environment to algorithms, such as global configuration options, paths or command lines parameters~\cite{jones1995}.)
%
%\begin{isabelle_cm}
%\isacommand{datatype}\ \isacharparenleft{\isacharprime}c\isacharcomma\ {\isacharprime}a\isacharparenright\ reader{\isacharunderscore}monad\ =\ Reader\ {\isacharparenleft}run{\isacharunderscore}reader\isacharcolon\ \isacartoucheopen{\isacharprime}c\ \isasymRightarrow\ {\isacharprime}a\isacartoucheclose\isacharparenright)\isanewline
%\isanewline
%\isacommand{definition}\ bind{\isacharunderscore}rd\ \isacommand{where}\ {\isacartoucheopen}bind{\isacharunderscore}rd\ m\ f\ \isacharequal\ Reader\ \isacharparenleft{\isasymlambda}r\isachardot\ run{\isacharunderscore}reader\ {\isacharparenleft}f\ {\isacharparenleft}run{\isacharunderscore}reader\ m\ r\isacharparenright\isacharparenright\ r\isacharparenright\isacartoucheclose\isanewline
%\isacommand{definition}\ return{\isacharunderscore}rd\ \isacommand{where}\ {\isacartoucheopen}return{\isacharunderscore}rd\ x\ \isacharequal\ Reader\ \isacharparenleft{\isasymlambda}{\isacharunderscore}\isachardot\ x\isacharparenright\isacartoucheclose\isanewline
%\isacommand{definition}\ get{\isacharunderscore}rd\ \isacommand{where}\ {\isacartoucheopen}get{\isacharunderscore}rd\ x\ \isacharequal\ Reader\ id\isacartoucheclose
%\end{isabelle_cm}
%With the definition above, we can define our sampling function with respect to the reader monad, where the environment is the entire set of coin flips, which is now a monad morphism (from the reader monad to the Giry monad):
%\begin{isabelle_cm}
%  sample{\isacharunderscore}rd\ \isacharequal\ sample\ \isasymcirc\ run{\isacharunderscore}reader
%\end{isabelle_cm}

