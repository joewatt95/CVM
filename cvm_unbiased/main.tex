\documentclass{article}
\usepackage[left=1.2in, right=1.2in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\newcommand{\prob}{\mathcal P}
\newcommand{\expectation}{\mathrm{E}}
\newcommand{\eps}{\varepsilon}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}

\begin{document}
\title{Unbiased CVM Algorithm}
\maketitle
\section{Notation}
For a finite set $S$, the probability space of uniformly sampling from the set is denoted by $U(S)$, i.e., for each $s \in S$ we have
$\prob_{U(S)}({s}) = |S|^{-1}$.
For the bernoulli probability space, over the set $\{\mathrm{true},\mathrm{false}\}$ we'll write $\mathrm{Ber}(p)$, i.e., $P_{\mathrm{Ber}(p)}(\{\mathrm{true}\}) = p$.
$I(P)$ is the indicator function for a predicate $P$, i.e., $I(\mathrm{true}) = 1$ and $I(\mathrm{false}) = 0$.
\section{Algorithm}
Let us fix 
\begin{itemize}
  \item $a_1,\ldots,a_l$ for the stream of elements, and
  \item $n$ for the maximal number of elements in the buffer, and
  \item $f$ for the fraction of elements to keep in the buffer, when we subsample.
\end{itemize}
Note that: $nf$ must be an integer $\frac{1}{2} \leq f < 1$.

\begin{algorithm}
\caption{Unbiased CVM algorithm}
\begin{algorithmic}
\State $\chi \gets \emptyset$; $p \gets 1$
\For{$i \gets 1$ to $l$}
  \State $\chi \gets \chi - \{a_i\}$
  \State $\mathrm{coin} \gets \mathrm{Ber}(p)$
  \If{$\mathrm{coin}$}
    \State $\chi \gets \chi \cup \{a_i\}$
  \EndIf
  \If{$|\chi| = n$}
    \State $\chi \gets U(\{ S \subseteq \chi \,|\, |S|=nf \})$
    \State $p \gets p f$
  \EndIf
\EndFor
\State \Return{$p^{-1} |\chi|$}
\end{algorithmic}
\end{algorithm}
We will denote the first five lines of the loop as step 1, the last four lines as step 2, and the distribution of the state of the algorithm after processing $i$ elements of the sequence by $\Omega_i$.
The elements of the probability spaces are pairs composed of a set and a fraction, representing $\chi$ and $p$. 
For example: $\Omega_0 = U(\{(\emptyset, 1)\})$ is the initial state, $\Omega_1 = U(\{(\{a_1\}, 1)\})$, etc. $\Omega_l$ denotes the final state. 
We introduce $\chi$ and $p$ as random variables defined over such probability spaces $\Omega$, in particular, $\chi$ (resp. $p$) is the projection to the first (resp. second) component.

The state of the algorithm after processing only step 1 of the $i$-th loop iteration is denoted by $\Omega'_i$.
So the sequence of states is represented by the distributions $\Omega_0, \Omega'_1, \Omega_1, \ldots, \Omega'_l, \Omega_l$.
A few easy-to-see observations without proof for any $0 \leq i \leq l$:
\begin{itemize}
\item $p(\omega) \leq 1$ a.s. (almost surely) for $\omega \in \Omega_i$,
\item $\chi(\omega) \subseteq \{a_1,\ldots,a_i\}$ a.s. for $\omega \in \Omega_i$,
\item $|\chi(\omega)| < n$ a.s. for $\omega \in \Omega_i$.
\end{itemize}
Also for the intermediate states, we have:
\begin{itemize}
\item $p(\omega) \leq 1$ a. s. for $\omega \in \Omega'_i$,
\item $\chi(\omega) \subseteq \{a_1,\ldots,a_i\}$ a.s. for $\omega \in \Omega'_i$,
\item $|\chi(\omega)| \leq n$ a.s. for $\omega \in \Omega'_i$.
\end{itemize}

\begin{lemma}
\label{le:neg_cor}
For all $k \in \{0,\ldots,l\}$, $S \subseteq \{a_1,..,a_k\}$:
\begin{align*}
  \expectation_{\Omega_k}\left[ \prod_{s \in S} p^{-1} I(s \in \chi) \right] & \leq 1 & 
  \expectation_{\Omega'_k}\left[ \prod_{s \in S} p^{-1} I(s \in \chi) \right] & \leq 1
\end{align*}
\end{lemma}
\begin{proof}
We show the result using induction over $k$. Note that we show the statement for arbitrary $S$, e.g., the induction statements are:
\begin{eqnarray*}
P(k) & :\leftrightarrow & \left(\forall S \subseteq \{a_1,..,a_k\}. \; \expectation_{\Omega_k}\left[p^{-|S|} I(S \subseteq \chi)\right] \leq 1\right) \\
Q(k) & :\leftrightarrow & \left(\forall S \subseteq \{a_1,..,a_k\}. \; \expectation_{\Omega'_k}\left[p^{-|S|} I(S \subseteq \chi)\right] \leq 1\right)
\end{eqnarray*}
and we will show $P(0),Q(1),P(1),Q(2),P(2),\ldots,Q(l),P(l)$ successively.
\paragraph{Induction start $P(0)$:} \phantom{.}\\
We have $p=1$ $\chi=\emptyset$ and $S \subseteq \emptyset$, and hence
\[
\expectation_{\Omega_0}\left[p^{-|S|} I(S \subseteq \chi)\right] = \expectation_{\Omega_0}\left[1^{-|\emptyset|} I(\emptyset \subseteq \emptyset))\right] = 1 \leq 1 \textrm{.}
\]
\paragraph{Induction step $P(k) \rightarrow Q(k+1)$:} \phantom{.}\\
Let $S \subseteq \{ a_1, \ldots, a_{k+1} \}$ and define $S' := S - \{ a_{k+1} \}$.
Note that $\Omega’_{k+1}$ can be constructed from $\Omega_k$ as a compound distribution, where $a_{k+1}$ is included in the buffer, with the probability $p$, which is itself a random variable over the space $\Omega_k$.

In particular, for example:
\[
\prob_{\Omega'_{k+1}}( P(\chi, p) ) = \int_{\Omega_k} \int_{Ber(p(\omega))} P(\chi(\omega)-\{a_{k+1}\}\cup f(\tau), p(\omega)) \, d \tau \, d \omega  
\]
for all predicates $P$ where we define $f(\mathrm{true}) = \{a_{k+1}\}$ and $f(\mathrm{false}) = \emptyset$.

We distinguish the two cases $a_{k+1} \in S$ and $a_{k+1} \notin S$.
\\
If $a_k \in S$ then:
\begin{eqnarray*}
  \expectation_{\Omega’_{k+1}}\left[\frac{I(S \subseteq \chi)}{p^{|S|}}\right] & = & \int_{\Omega_k} \frac{I(S' \subseteq \chi(\omega))}{ p(\omega)^{|S'|+1}}
  \int_{\mathrm{Ber}(p(\omega))} I(\tau) \, d \tau \, d \omega \\ 
  & = & \int_{\Omega_k} p(\omega)^{-(|S'|+1)} I(S' \subseteq \chi(\omega)) p(\omega) \, d \omega \\
  & = & \expectation_{\Omega_k}\left[p^{-|S'|} I(S' \subseteq \chi)\right] \leq_{IH} 1
\end{eqnarray*}
If $a_k \notin S$ then:
\begin{eqnarray*}
  \expectation_{\Omega’_{k+1}}\left[\frac{I(S \subseteq \chi)}{p^{|S|}}\right] & = & \int_{\Omega_k} \frac{I(S' \subseteq \chi(\omega))}{p(\omega)^{|S'|+1}}
  \int_{\mathrm{Ber}(p(\omega))} 1 \, d \tau \, d \omega \\ 
  & = & \int_{\Omega_k} p(\omega)^{-(|S'|)} I(S' \subseteq \chi(\omega)) \, d \omega \\
  & = & \expectation_{\Omega_k}\left[p^{-|S'|} I(S' \subseteq \chi)\right] \leq_{IH} 1
\end{eqnarray*}
\paragraph{Induction step $Q(k+1) \rightarrow P(k+1)$:} \phantom{.}\\
Let $S \subseteq \{ a_1, \ldots, a_{k+1} \}$.
Let us also introduce the abbreviation $C(X) := \{T \subset X \, | \, |T| = n f \}$, i.e., the set of $nf$-subsets of $X$.

First we estimate the probability that $S$ is in a random subset of $C(T)$:
\begin{equation}
  \label{eq:prob_c_t}
  \int_{U(C(T))} I(S \subseteq \tau) \, d \tau = I(S \subseteq T) \binom{|T-S|}{nf-|S|} \binom{|T|}{nf}^{-1} \leq I(S \subseteq T) \left(\frac{nf}{|T|}\right)^{|S|}
\end{equation}
For the above it is best to consider the two cases $S \subseteq T$ and the converse separately. Note that if $S \subseteq \tau$ for $\tau \in C(S)$ then it must be true that: 
$S \subseteq T$. 

Let us again note that $\Omega_{k+1}$ is a compound distribution over $\Omega_k$. In general, for all predicates $P$:
\[
  \prob_{\Omega_{k+1}}( P(\chi, p) )  = \int_{\Omega'_{k+1}} I( |\chi(\omega)|<n ) P(\chi(\omega),p(\omega)) + I( |\chi(\omega)|=n ) \int_{U(\chi(\omega))} P(\tau, f p(\omega)) \, d \tau  \, d \omega \textrm{.}
\]

With this we can can now verify the induction step:
\begin{eqnarray*}
&  & \textstyle\expectation_{\Omega_{k+1}}\left[p^{-|S|} I(S \subseteq \chi)\right]  \\
& = &\textstyle \int_{\Omega'_{k+1}} \frac{I(|\chi(\omega)| < n)}{p(\omega)^{|S|}} I(S \subseteq \chi(\omega)) \, d \omega 
+ \int_{\Omega'_{k+1}} \frac{I(|\chi(\omega)| = n)}{(p(\omega)f)^{|S|}} \int_{U(C(\chi(\omega)))} I(S \subseteq \tau) d \tau \, d \omega  \\
& \underset{\textrm{Eq.~\ref{eq:prob_c_t}}}{\leq} & \textstyle \int_{\Omega'_{k+1}} \frac{I(|\chi(\omega)| < n)}{p(\omega)^{|S|}} I(S \subseteq \chi(\omega)) \, d \omega
+ \int_{\Omega'_{k+1}} \frac{I(|\chi(\omega)| = n)}{(p(\omega)f)^{|S|}} I(S \subseteq \chi(\omega)) \left(\frac{nf}{|\chi(\omega)|}\right)^{|S|} \, d \omega  \\
& = & \textstyle \int_{\Omega'_{k+1}} \frac{I(|\chi(\omega)| = n)+I(|\chi(\omega)| < n)}{p(\omega)^{|S|}} I(S \subseteq \chi(\omega)) \, d \omega \\
& = & \textstyle \expectation_{\Omega'_{k+1}}\left[p^{-|S|} I(S \subseteq \chi) \right] \leq_{IH} 1
\end{eqnarray*}

\end{proof}
The dual version of Lemma~\ref{le:neg_cor} would be: $\expectation[ (1-p)^{-|S|} I(S \cap \chi=\emptyset) ] \leq 1$, but this is somewhat undefined for $p=1$.
This can easily be resolved, by taking into account that the first sub-sampling step happens deterministically, exactly at the iteration $k$, when $\{a_1,\ldots,a_k\} = n$,
i.e., we have $p \leq f < 1$ a.s. for $\Omega_l$ if $\{a_1,\ldots,a_l\} \geq n$ and $p=1$ a.s. otherwise.

\begin{lemma}
\label{le:neg_cor_2}
If $\{a_1,\ldots,a_l\} \geq n$, then for all $S \subseteq \{a_1,\ldots,a_l\}$:
\[
\expectation_{\Omega_l} \left[ (1-p)^{-|S|} I(S \cap \chi=\emptyset) \right] \leq 1 \textrm{.}
\]
\end{lemma}
\begin{proof}
The proof is analogous to the proof of Lemma~\ref{le:neg_cor} using induction (with special handling for the $p=1$ case).
\end{proof}
  
\section{Concentration}
This section establishes that the result of the algorithm is concentrated around the cardinality of $A = \{ a_1, \ldots, a_l \}$. This will be done by Chernoff bounds for the probability that the estimate is above $(1+\eps)|A|$ (resp. below $(1-\eps)|A|$ assuming $p$ is not too small and a tail estimate for $p$ being too small.

It should be noted that concentration is trivial, if $|A| \leq n$, i.e., if we never need to do sub-sampling.
So we assume $|A| \geq n$. 

We define $q := n/(4|A|)$ - notice that $q \leq \frac{1}{4}$. Moreover we'll introduce the notation $\tilde{p} := \max(p,q)$.
Basically $\tilde{p}$ is a clamped version of $p$, and in particular, $\tilde{p}^{-1}$ is bounded from above by $q^{-1}$.

The following is a helper lemma:
\begin{lemma}
\label{le:helper}
For any $\Omega \in \{\Omega_0,\ldots,\Omega_l\} \cup \{\Omega'_1,\ldots,\Omega'_l\}$ and $0 < \eps \leq 1$:
\[
\prob_{\Omega} \left( \tilde{p}^{-1} |\chi| \geq (1+\eps) |A| \right) \leq \exp\left(-\frac{n}{12} \eps^2\right)
\]
\end{lemma}
\begin{proof}
By assumption there exists a $k$ such that $\Omega \in \{\Omega_k, \Omega'_k\}$. Let $A' = A \cap \{a_1,\ldots,a_k\}$.

Moreover, we define 
\[
  a(t) := \sup_{p \in (0,1]} p (e^{t/ \tilde{p}}-1) = q (e^{t/q}-1)
\]

To get a tail estimate, we use the Cram\'{e}r-Chernoff method: 
{\allowdisplaybreaks
\begin{eqnarray*}
  \prob_{\Omega} \left( \tilde{p}^{-1} |\chi| \geq (1+\eps) |A| \right) & \leq & \prob_{\Omega} \left( \tilde{p}^{-1} |\chi| \geq (1+\eps) |A'| \right) \\
  & \leq & \exp( -t (1+\eps) |A| ) \expectation_\Omega \left[ \exp ( t \tilde{p}^{-1} |\chi| ) \right] \\
 & = & \exp( -t (1+\eps) |A| ) \expectation_\Omega \left[ \prod_{s \in A'} \exp ( t \tilde{p}^{-1} I(s \in \chi) ) \right] \\  
  & = & \exp( -t (1+\eps) |A| ) \expectation_\Omega \left[ \prod_{s \in A'} (1 + I(s \in \chi) (e^{t \tilde{p}^{-1}}-1) ) \right] \\  
 & \leq & \exp( -t (1+\eps) |A| ) \expectation_\Omega \left[ \prod_{s \in A'} \left(1 + \frac{a(t)}{p} I(s \in \chi) \right) \right] \\  
 & = & \exp( -t (1+\eps) |A| ) \sum_{T \subseteq A'} a(t)^{|T|} \expectation_\Omega \left[ \prod_{s \in T} (p^{-1} I(s \in \chi) ) \right] \\  
 & \underset{Lemma~\ref{le:neg_cor}}{\leq} & \exp( -t (1+\eps) |A'| ) \sum_{T \subseteq A'} a(t)^{|T|} \\  
 & = & \left(\exp( -t (1+\eps) |A| ) (1+a(t))\right)^{|A'|} \\ & \leq & \exp ( |A| (\ln(1+a(t))-t (1+\eps)))
\end{eqnarray*}}
So we just need to show that (using $|A|=\frac{n}{4q}$) for some $t > 0$:
\[
  \ln(1+a(t))-t (1+\eps) \leq \frac{-q \eps^2}{3}
\]
which can be verified for $t = q \ln ( 1 + \frac{\eps}{1-q(1+\eps)}) > 0$.
The latter can be established by analyzing the function $f(q,\eps) = -\ln (1+a(t)) +t (1+\eps) - \frac{q\eps^2}{3}$.
For which it is easy to check $f(q,0) = 0$ and the derivative with respect to $\eps$ is positive in the range $0 \leq q \leq 1/4$ and $0 < \eps \leq 1$, i.e.,
$f(q,\eps) \geq 0$.
\end{proof}

Using the previous we can estimate bounds for $p$ becoming too small, as well as, an upper tail bound.
\begin{lemma}\label{le:low_p}
\[
\prob_{\Omega_l}(p < q) \leq l \exp\left( - \frac{n}{12}\right)
\]
\end{lemma}
\begin{proof}
We'll use a similar strategy as in the $\mathrm{Bad}_2$ bound in the original CVM paper. 
Let $j$ be maximal, s.t., $q \leq f^j$.

Hence $f^{j+1} < q$ and: 
\begin{equation}
\label{eq:f_j}
  f^j \leq 2f f^j < 2q = \frac{n}{2|A|} \textrm{.}
\end{equation}

First, we bound the probability of jumping from $p=f^j$ to $p=f^j+1$ at a specific point in the algorithm, e.g., after processing $k$ stream elements.
It can only happen if $|\chi|=n$, $p=f^j$ in $\Omega'_k$.

Then
\begin{eqnarray*}
  \prob_{\Omega'_k} ( |\chi| \geq n \wedge p=f^j) & \leq & \prob( \tilde{p}^{-1} |\chi| \geq f^{-j} n ) \\
    & \underset{Eq.~\ref{eq:f_j}}{\leq} & \prob( \tilde{p}^{-1} |\chi| \geq 2|A| ) \\
    & \underset{Lem.~\ref{le:helper}}{\leq} & \exp( - n/12)
\end{eqnarray*}

The probability that this happens ever in the entire process is then at most $l$ times the above which proves the lemma.
\end{proof}

Now we come to the tail bounds. The upper tail bound follows directly from Lemma~\ref{le:helper}:
\begin{lemma}\label{le:upper_tail}
Let $0 < \eps \leq 1$ then:
\[
  L = \prob_{\Omega_l} ( p^{-1} |\chi| \geq (1+\eps)|A| \wedge p \geq q) \leq \exp\left(-\frac{n}{12} \eps^2\right)
\]
\end{lemma}
\begin{proof}
\begin{eqnarray*}  
  L & =&  \prob_{\Omega_l} ( \tilde{p}^{-1} |\chi| \geq (1+\eps)|A| \wedge p \geq q) \\
  & \leq & \prob_{\Omega_l} ( \tilde{p}^{-1} |\chi| \geq (1+\eps)|A| ) \\
  & \underset{Lemma~\ref{le:helper}}{\leq}  &  \exp\left(-\frac{n}{12} \eps^2\right)
\end{eqnarray*}
\end{proof}

\begin{lemma}\label{le:lower_tail}
Let $0 < \eps \leq 1$ then:
\[
  L := \prob_{\Omega_l} ( p^{-1} |\chi| \leq (1-\eps)|A| \wedge p \geq q) \leq \exp\left(-\frac{n}{12} \eps^2\right)
\]
\end{lemma}
\begin{proof}
Let us define 
\begin{align*}
  a & := \frac{\eps}{1-\eps} &
  t & := (1-q+\eps q) \ln \left(1 + \frac{\eps}{(1-q)(1-\eps)} \right) \\
  f(p) & = \begin{cases} 1-p+\eps p & \textrm{ if } p \geq q \\ \frac{t}{\ln \left(1+\frac{3}{4}a\right)} & \textrm{ otherwise.} \end{cases}
\end{align*}

We first observe that $t > 0$. Moreover, 
\begin{equation}\label{eq:f_p}
  \exp\left( \frac{t}{f(p)}\right)-1 \leq a (1-p) \textrm{ for } 0 < p < 1  
\end{equation}

With these definitions we again follow the Cram\'{e}r-Chernoff method: 
{\allowdisplaybreaks
\begin{eqnarray*}
  L & = & \prob_{\Omega_l} \left( |A-\chi| \geq (1-(1-\eps)p) |A| \wedge p \geq q \right) \\
    & = & \prob_{\Omega_l} \left( |A-\chi| \geq f(p)|A| \wedge p \geq q \right) \\
    & \leq & \prob_{\Omega_l} \left( |A-\chi| \geq f(p)|A| \right)  \\
    & \leq & \exp( -t |A| ) \expectation_{\Omega_l} \left[ \exp \left( \frac{t}{f(p)} |A-\chi| \right) \right] \\
    & = & \exp( -t |A| ) \expectation_{\Omega_l} \left[ \prod_{s \in A} 1 + (e^{t/f(p)} - 1) I(s \notin \chi) \right] \\
    & \underset{Eq.~\ref{eq:f_p}}{\leq}  & \exp( -t |A| ) \sum_{T \subseteq A} \expectation_{\Omega_l} \left[ \prod_{s \in T} \frac{a}{1-p} I(s \notin \chi) \right] \\
    & \underset{Lemma~\ref{le:neg_cor_2}}{\leq} & \exp( -t |A| ) (1+a)^{|A|}
\end{eqnarray*}
}

Substituting $t$ and $a$ and using $|A| = \frac{q}{4n}$, we can see that the lemma is true if
\[
  f(q,\eps) = (1-q+\eps q) \ln\left(1+\frac{\eps}{(1-q)(1-\eps)}\right) - \ln\left(1+\frac{\eps}{1-\eps}\right) - \frac{q}{3} \eps^2
\]
is non-negative for $0 \leq q \leq \frac{1}{4}$ and $0 < \eps < 1$.
This can be verified by checking that $f(q,0) = 0$ and that the derivative with respect to $\eps$ is non-negative.
\end{proof}
We can now establish the concentration result:
\begin{theorem}
Let $0 < \eps < 1$ and $0 < \delta < 1$ and $n \geq \frac{12}{\eps^2} \ln\left(\frac{3l}{\delta}\right)$ then:
\[
  L = \prob_{\Omega_l} \left( | p^{-1} |\chi| - |A| | \geq \eps |A| \right) \leq \delta
\]
\end{theorem}
\begin{proof}
Note that the theorem is trivial if $|A| < n$. If not:
\begin{eqnarray*}
  L & \leq & \prob_{\Omega_l} \left( | p^{-1} |\chi| \leq (1-\eps) |A| \wedge p \geq q \right) + 
    \prob_{\Omega_l} \left( | p^{-1} |\chi| \leq (1+\eps) |A| \wedge p \geq q \right) + 
    \prob_{\Omega_l} \left( p < q \right) \\
    & \underset{Le.~\ref{le:low_p}-\ref{le:upper_tail}}{\leq} &
    \exp\left( - \frac{n}{12} \eps^2 \right)  + \exp\left( - \frac{n}{12} \eps^2 \right) + l \exp\left( - \frac{n}{12} \right) \\
    & \leq & \frac{\delta}{3} + \frac{\delta}{3} + \frac{\delta}{3} 
\end{eqnarray*}
\end{proof}

\section{Unbiasedness}
It is easy to conclude using Lemma~\ref{le:neg_cor} and \ref{le:neg_cor_2} that for each $s \in A$ that:
$\expectation_{\Omega_l} [ p^{-1} I(s \in S) ] = 1$.
By linearity of expectation we can conclude that
\[
  \expectation_{\Omega_l} [ p^{-1} |\chi| ] = \sum_{s \in A} \expectation_{\Omega_l} [ p^{-1} I(s \in S) ] = |A| \textrm{.}
\]
\end{document}

